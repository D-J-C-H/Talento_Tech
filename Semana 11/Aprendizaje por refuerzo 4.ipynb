{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aebf19-0f6c-4717-86e4-fa504cc715f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b291c367-ca8e-4fff-a3b1-2d5469bd1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffae620c-5ea5-4dde-bcc2-565b8f951cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del gridworld\n",
    "gridworld = np.array([\n",
    "    [-1, -1, -1, 1],\n",
    "    [-1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95939c7-4832-4714-9ce6-3b5677b95b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de Q-Learning\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "909f6d85-ac5c-47b5-89a5-73c840ad5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de las acciones posibles: arriba, abajo, izquierda, derecha\n",
    "acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c87d394f-3dce-46a8-bec9-55ca326da7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de Q-Learning\n",
    "\n",
    "# Inicialización de Q\n",
    "Q = np.zeros((gridworld.shape[0], gridworld.shape[1], len(acciones)))\n",
    "for _ in range(num_episodes):\n",
    "    estado = (0, 0)  # Estado inicial\n",
    "    while estado != (0, 3):  # Asumamos que (0, 3) es el estado terminal\n",
    "        accion = np.random.choice(range(len(acciones)))\n",
    "        nueva_fila = estado[0] + acciones[accion][0]\n",
    "        nueva_col = estado[1] + acciones[accion][1]\n",
    "\n",
    "        # Asegúrate de que la nueva posición esté dentro del gridworld\n",
    "        if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_col < gridworld.shape[1]:\n",
    "            recompensa = gridworld[nueva_fila, nueva_col]\n",
    "            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n",
    "            Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n",
    "            estado = (nueva_fila, nueva_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd20b16-fb9b-433f-901c-042e14ce6bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values después del entrenamiento: \n",
      "[[[ 0.   -1.    0.   -1.  ]\n",
      "  [-1.   -0.1   0.   -1.9 ]\n",
      "  [-1.    1.    0.   -1.09]\n",
      "  [ 0.    0.    0.    0.  ]]\n",
      "\n",
      " [[ 0.   -1.9  -1.   -1.  ]\n",
      "  [-1.   -1.09 -1.   -1.9 ]\n",
      "  [-1.9  -0.1  -0.1  -1.9 ]\n",
      "  [-1.09  0.    1.   -1.  ]]\n",
      "\n",
      " [[ 0.   -1.9  -1.   -1.  ]\n",
      "  [-1.   -1.9  -1.9  -1.  ]\n",
      "  [-1.9  -1.   -1.09 -1.  ]\n",
      "  [-1.9   0.   -0.1  -1.  ]]\n",
      "\n",
      " [[ 0.   -1.   -1.    0.  ]\n",
      "  [-1.   -1.   -1.9   0.  ]\n",
      "  [-1.   -1.   -1.9   0.  ]\n",
      "  [-1.    0.   -1.    0.  ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q-values después del entrenamiento: \")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8487981b-3e65-4890-9e1c-47936d4bfd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del entorno de navegación (datos ficticios)\n",
    "entorno = np.array([\n",
    "[0, 0, 0, 0, 0],\n",
    "[0, -1, -1, -1, 0],\n",
    "[0, 0, -1, 0, 0],\n",
    "[0, -1, -1, -1, 0],\n",
    "[0, 0, 0, 0, 0]\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11b084ee-4e09-481b-b817-d3a98a2085e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de acciones posibles (arriba, abajo, izquierda, derecha)\n",
    "acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11360d44-4d8a-4d8b-b640-5b4103827daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de Q-Learning\n",
    "Q = np.zeros((entorno.shape[0], entorno.shape[1], len (acciones)))\n",
    "gamma = 0.9 # Factor de descuento\n",
    "alpha = 8.1 #Tasa de aprendizaje\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53858045-537d-4659-a628-612b92ea7774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deyvi.caicedo\\AppData\\Local\\Temp\\ipykernel_35292\\2601618761.py:13: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n",
      "C:\\Users\\deyvi.caicedo\\AppData\\Local\\Temp\\ipykernel_35292\\2601618761.py:13: RuntimeWarning: invalid value encountered in scalar add\n",
      "  Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento de Q-Learning\n",
    "for _ in range(num_episodes):\n",
    "    estado = (0, 0)  # Estado inicial\n",
    "    while estado != (0, 3):  # Corrección aquí\n",
    "        accion = np.random.choice(range(len(acciones)))\n",
    "        nueva_fila = estado[0] + acciones[accion][0]\n",
    "        nueva_col = estado[1] + acciones[accion][1]\n",
    "\n",
    "        # Asegúrate de que la nueva posición esté dentro del gridworld\n",
    "        if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_col < gridworld.shape[1]:\n",
    "            recompensa = gridworld[nueva_fila, nueva_col]\n",
    "            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n",
    "            Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n",
    "            estado = (nueva_fila, nueva_col)\n",
    "            \n",
    "            if recompensa == 1:  # Suponemos que el objetivo es llegar a la recompensa de 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ba124aa-e5fa-487f-99f6-5658c0dccf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores Q después del entrenamiento:\n",
      "[[[ 0. nan  0. nan]\n",
      "  [nan nan  0. nan]\n",
      "  [nan inf  0. nan]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0. nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan  0. inf nan]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0. nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan  0. nan nan]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0. nan nan  0.]\n",
      "  [nan nan nan  0.]\n",
      "  [nan nan nan  0.]\n",
      "  [nan  0. nan  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores Q después del entrenamiento:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03e7fe07-5ea4-4caa-b4db-fa20da097895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de los estados (niveles de inventario), acciones (órdenes de reabastecimiento) y recompensas (costos, ganancias, etc.)\n",
    "estados = ['Bajo', 'Medio', 'Alto']\n",
    "acciones = ['Reabastecer', 'No reabastecer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3389819b-29e7-4a7c-b6ba-2a9355f45265",
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensas = {\n",
    "        ('Bajo', 'Reabastecer'): 50, \n",
    "        ('Bajo', 'No reabastecer'): -10,\n",
    "        ('Medio', 'Reabastecer'): 38,\n",
    "        ('Medio', 'No reabastecer'): 0,\n",
    "        ('Alto', 'Reabastecer'): 10,\n",
    "        ('Alto', 'No reabastecer'): -20\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5939c3f2-317c-4329-b442-a1f827fe6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de Q-Learning\n",
    "Q = {}\n",
    "gamma = 0.9 # Factor de descuento\n",
    "alpha = 0.1 # Tasa de aprendizaje\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51a72e65-c162-4078-9f43-a035b55e4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_episodes):\n",
    "    estado_actual = np.random.choice(estados)\n",
    "    while True:\n",
    "        accion = np.random.choice(acciones)\n",
    "        recompensa = recompensas.get((estado_actual, accion), 0)  # Manejo de casos donde (estado, accion) no está en recompensas\n",
    "\n",
    "        # Inicializar Q para el estado y acción si no existe\n",
    "        if estado_actual not in Q:\n",
    "            Q[estado_actual] = {}\n",
    "        if accion not in Q[estado_actual]:\n",
    "            Q[estado_actual][accion] = 0\n",
    "\n",
    "        nuevo_estado = np.random.choice(estados)\n",
    "\n",
    "        # Obtener el máximo valor de Q para el nuevo estado\n",
    "        if nuevo_estado in Q and Q[nuevo_estado]:\n",
    "            max_nuevo_estado = max(Q[nuevo_estado].values())\n",
    "        else:\n",
    "            max_nuevo_estado = 0\n",
    "\n",
    "        # Actualización de la tabla Q\n",
    "        Q[estado_actual][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado_actual][accion])\n",
    "\n",
    "        estado_actual = nuevo_estado\n",
    "        \n",
    "        # Condición de terminación\n",
    "        if recompensa in {50, 30, 10}:  # Usar conjunto para verificar múltiples valores\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e6350a3-7027-4f55-8204-8e4bbcf6b70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores Q después del entrenamiento:\n",
      "{'Alto': {'No reabastecer': 271.045912557071, 'Reabastecer': 307.0983461425744}, 'Bajo': {'No reabastecer': 289.37241507833494, 'Reabastecer': 342.7698327339255}, 'Medio': {'Reabastecer': 325.28939043045676, 'No reabastecer': 296.79031392564985}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores Q después del entrenamiento:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7335c4-8304-4f6d-ae62-3f1f3599b1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
