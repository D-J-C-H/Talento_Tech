{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3edbf-33b3-4c1e-9d36-4c64f301d375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7103e82-7fb6-4ed7-bd27-a35e5e12e6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d80c92a9-989b-453b-b674-3e66d6bb9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  # Importar el módulo random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49fe40de-a211-461e-8327-2616afcd2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteRL:\n",
    "    def __init__(self, acciones):\n",
    "        self.acciones = acciones\n",
    "\n",
    "    def seleccionar_accion(self, estado):\n",
    "        # Ejemplo de selección aleatoria de acción\n",
    "        return random.choice(self.acciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11f979f0-6902-424f-a827-941c516981ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción seleccionada por el agente: abajo\n"
     ]
    }
   ],
   "source": [
    "# Uso del agente RL\n",
    "acciones_posibles =['izquierda', 'derecha', 'arriba', 'abajo']\n",
    "agente = AgenteRL (acciones_posibles)\n",
    "estado_actual = [0, 0] # Estado inicial del entorno\n",
    "accion_seleccionada = agente.seleccionar_accion (estado_actual)\n",
    "print(\"Acción seleccionada por el agente:\", accion_seleccionada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a2b5a2b-5a70-4ba9-b0cd-691d8ae0e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntornoRL:\n",
    "        def __init__(self, estados):\n",
    "            self.estados = estados\n",
    "            \n",
    "        def tomar_accion(self, accion):\n",
    "# Simulación de la transición de estado\n",
    "            nuevo_estado = random.choice (self.estados)\n",
    "            recompensa = random.randint(-10, 10)\n",
    "            return nuevo_estado, recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "034e88f2-c357-43b0-9480-051ed8270f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo estado:  C\n",
      "Recompensa recibida: -8\n"
     ]
    }
   ],
   "source": [
    "# Uso del entorno RL\n",
    "estados_posibles = ['A', 'B', 'C', 'D']\n",
    "entorno = EntornoRL (estados_posibles)\n",
    "accion = 'izquierda' # Acción seleccionada por el agente\n",
    "nuevo_estado, recompensa = entorno.tomar_accion (accion)\n",
    "print(\"Nuevo estado: \", nuevo_estado)\n",
    "print(\"Recompensa recibida:\", recompensa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d41f232c-a4d7-4961-b00b-2d30c32cf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, estados, acciones, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.estados = estados\n",
    "        self.acciones = acciones\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma  # Corregido: 'gara' a 'gamma'\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def actualizar_q_table(self, estado_actual, accion, recompensa, nuevo_estado):\n",
    "        if estado_actual not in self.q_table:\n",
    "            self.q_table[estado_actual] = {a: 0 for a in self.acciones}  # Corregido: '=' en lugar de '{}'\n",
    "        if nuevo_estado not in self.q_table:\n",
    "            self.q_table[nuevo_estado] = {a: 0 for a in self.acciones}  # Corregido: '=' en lugar de '{}'\n",
    "\n",
    "        q_actual = self.q_table[estado_actual][accion]  # Corregido: '=' en lugar de 'self.q_actual'\n",
    "        max_q_nuevo_estado = max(self.q_table[nuevo_estado].values())  # Corregido: 'max_q_nuevo_estado' y 'self.q.table' a 'self.q_table'\n",
    "\n",
    "        # Calcular el nuevo valor Q\n",
    "        nuevo_q_valor = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo_estado - q_actual)  # Corregido: sintaxis y nombres de variables\n",
    "\n",
    "        # Actualizar la tabla Q\n",
    "        self.q_table[estado_actual][accion] = nuevo_q_valor  # Corregido: '=' en lugar de 'nuevo.q.valor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9826c718-172d-47b1-b1e1-f752f569a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, estados, acciones, alpha = 0.1, gamma = 0.9, epsilon = 0.1):\n",
    "        self.estados = estados\n",
    "        self.acciones = acciones\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def actualizar_q_table(self, estado_actual, accion, recompensa,nuevo_estado):\n",
    "        if estado_actual not in self.q_table:\n",
    "            self.q_table[estado_actual] = {a: 0 for a in self.acciones}\n",
    "        if nuevo_estado not in self.q_table:\n",
    "            self.q_table[nuevo_estado] = {a: 0 for a in self.acciones}\n",
    "\n",
    "\n",
    "    # Obtener el valor Q actual y el máximo valor Q en el nuevo estado\n",
    "        q_actual = self.q_table[estado_actual][accion]\n",
    "        max_q_nuevo_estado = max(self.q_table[nuevo_estado].values())\n",
    "\n",
    "        # Calcular el nuevo valor Q\n",
    "        nuevo_q_valor = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo_estado - q_actual)\n",
    "\n",
    "        # Actualizar la tabla Q con el nuevo valor calculado\n",
    "        self.q_table[estado_actual][accion] = nuevo_q_valor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "137d271d-964a-473f-81bc-169f84f6bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso del algoritmo Q-Learning\n",
    "estados = ['A', 'B', 'C']\n",
    "acciones = ['izquierda', 'derecha']\n",
    "q_learning = QLearning (estados, acciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89e9e554-e541-4b53-ba32-e9ac471544f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulación de una época de entrenamiento\n",
    "estado_actual = 'A'\n",
    "accion = 'izquierda'\n",
    "nuevo_estado = 'B'\n",
    "recompensa = 10\n",
    "q_learning.actualizar_q_table(estado_actual, accion, recompensa, nuevo_estado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ffe28cd-ace9-4caf-9b86-43f55312579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Q actualizada:\n",
      "{'A': {'izquierda': 1.0, 'derecha': 0}, 'B': {'izquierda': 0, 'derecha': 0}}\n"
     ]
    }
   ],
   "source": [
    "#Visualización de la tabla Q\n",
    "print(\"Tabla Q actualizada:\")\n",
    "print(q_learning.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2343a0e-01fe-427f-8aec-8b0da8d2ccf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
