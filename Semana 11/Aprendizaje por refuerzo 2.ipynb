{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6de968-f5ed-482b-baa5-a614819d7b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c92bc6-1c7b-4c4d-bab8-893dff7cfc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3b6248-e504-4b73-850a-6c955ae5ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02047f0a-3800-45f6-b1b0-57044e5945ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de estados, acciones y recompensas\n",
    "estados = ['A', 'B', 'C']\n",
    "acciones = ['Arriba', 'Abajo']\n",
    "recompensas = np.random.randint(0, 10, size=(len(estados),len(acciones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35939872-4924-4ada-a1fb-67ecff8a85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de transición aleatoria\n",
    "def transicion_aleatoria():\n",
    "    return np.random.choice(estados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2299b503-0e1f-40d0-b51d-2fea37b926d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: A\n",
      "Acción tomada: Arriba\n",
      "Nuevo estado: A\n",
      "Recompensa: 9\n"
     ]
    }
   ],
   "source": [
    "# Generación de datos\n",
    "estado_actual = np.random.choice(estados)\n",
    "accion= np.random. choice (acciones)\n",
    "nuevo_estado = transicion_aleatoria()\n",
    "recompensa = recompensas[estados.index (estado_actual),acciones.index(accion)]\n",
    "print(\"Estado actual:\", estado_actual)\n",
    "print(\"Acción tomada:\", accion)\n",
    "print(\"Nuevo estado:\", nuevo_estado)\n",
    "print(\"Recompensa:\", recompensa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e23b1a8-dd83-4114-9c2a-9cb969edad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n",
    "    # Inicializar los valores de los estados\n",
    "    valores = {estado: 8 for estado in mdp.estados}\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for estado in mdp.estados:\n",
    "            valor_previo = valores[estado]\n",
    "            # Calcular el nuevo valor del estado\n",
    "            valores[estado] = max(\n",
    "                sum(\n",
    "                    mdp.transiciones[estado][accion][nuevo_estado] * \n",
    "                    (mdp.recompensas[estado][accion][nuevo_estado] + gamma * valores[nuevo_estado])\n",
    "                    for nuevo_estado in mdp.estados\n",
    "                )\n",
    "                for accion in mdp.acciones\n",
    "            )\n",
    "            # Actualizar delta\n",
    "            delta = max(delta, abs(valor_previo - valores[estado]))\n",
    "\n",
    "        # Salir del bucle si delta es menor que theta\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return valores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f691dc7-0ffc-43dd-a993-1a49bed96435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, estados, acciones, transiciones, recompensas):\n",
    "        self.estados = estados\n",
    "        self.acciones = acciones\n",
    "        self.transiciones = transiciones\n",
    "        self.recompensas = recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c38b385-c7e9-4d05-91dd-0388e4780c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n",
    "    valores = {estado: 8 for estado in mdp.estados}  # Inicializar con un valor arbitrario\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for estado in mdp.estados:\n",
    "            valor_previo = valores[estado]\n",
    "            valores[estado] = max(\n",
    "                sum(\n",
    "                    mdp.transiciones[estado][accion].get(nuevo_estado, 0) * \n",
    "                    (mdp.recompensas[estado][accion].get(nuevo_estado, 0) + gamma * valores[nuevo_estado])\n",
    "                    for nuevo_estado in mdp.estados\n",
    "                )\n",
    "                for accion in mdp.acciones\n",
    "            )\n",
    "            delta = max(delta, abs(valor_previo - valores[estado]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5e75d6-e64f-400c-81e3-5deb8a1c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_propiedad_markov(mdp):\n",
    "    for estado in mdp.estados:\n",
    "        for accion in mdp.acciones:\n",
    "            suma_probabilidades = sum(mdp.transiciones[estado][accion].values())\n",
    "            if not np.isclose(suma_probabilidades, 1):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb032095-030c-4bd5-ae65-bd1ed2fdc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_recompensa_promedio(mdp):\n",
    "    recompensa_total = 0\n",
    "    total_acciones = 0\n",
    "    for estado in mdp.estados:\n",
    "        for accion in mdp.acciones:\n",
    "            for nuevo_estado in mdp.estados:\n",
    "                recompensa_total += mdp.recompensas[estado][accion].get(nuevo_estado, 0)\n",
    "                total_acciones += 1\n",
    "    return recompensa_total / total_acciones if total_acciones > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19cc6b2a-1f17-4f5e-a1b3-aa4fa5304578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el objeto MDP\n",
    "estados = ['A', 'B']\n",
    "acciones = ['izquierda', 'derecha']\n",
    "transiciones = {\n",
    "    'A': {'izquierda': {'A': 0.8, 'B': 0.2}, 'derecha': {'A': 0.7, 'B': 0.3}},\n",
    "    'B': {'izquierda': {'A': 0.6, 'B': 0.4}, 'derecha': {'A': 0.5, 'B': 0.5}}\n",
    "}\n",
    "recompensas = {\n",
    "    'A': {'izquierda': {'A': 1, 'B': 2}, 'derecha': {'A': 3, 'B': 4}},\n",
    "    'B': {'izquierda': {'A': 2, 'B': 3}, 'derecha': {'A': 4, 'B': 5}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bbf391-b5c0-47cc-8d0a-b7efebb862a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una instancia del objeto MDP\n",
    "mdp = MDP(estados, acciones, transiciones, recompensas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a0d58a9-a52c-4a1d-8da8-ccb12aace7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de los estados: {'A': 36.879692616851145, 'B': 38.34868116557287}\n",
      "Cumple con la propiedad de Markov: True\n",
      "Recompensa promedio por acción: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "valores_estados = calcular_valor_estado(mdp)\n",
    "print(\"Valores de los estados:\", valores_estados)\n",
    "\n",
    "print(\"Cumple con la propiedad de Markov:\", verificar_propiedad_markov(mdp))\n",
    "\n",
    "print(\"Recompensa promedio por acción:\", calcular_recompensa_promedio(mdp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
