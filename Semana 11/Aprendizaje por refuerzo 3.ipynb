{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9999f6-77b8-4fa3-b6de-ce206e5311d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd12b93-b8f2-4fcb-bc8e-2d041faa64e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec81adf6-f8a9-4889-b040-15b817fc8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: Introducción a los principales algoritmos de RL\n",
    "#Define el entorno del juego\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state_space = [0, 1, 2, 3] # Estados posibles\n",
    "        self.action_space = [0, 1] # Acciones posibles \n",
    "        self.rewards = {0:-1, 1: -1, 2: -1, 3: 10} # Recompensas por estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02c4019-0195-424f-80d6-d7a230825e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crea una instancia del entorno\n",
    "env = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a37575-aa3a-4964-8e23-ddf90fbb4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estados: [0, 1, 2, 3]\n",
      "Acciones: [0, 1]\n",
      "Recompensas: {0: -1, 1: -1, 2: -1, 3: 10}\n"
     ]
    }
   ],
   "source": [
    "# Muestra información del entorno\n",
    "print(\"Estados:\", env.state_space)\n",
    "print(\"Acciones:\", env.action_space)\n",
    "print(\"Recompensas:\", env.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff9dc28-f881-4f21-998d-485372b0f126",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Ejercicio 2: Q-Learning\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95477475-96aa-45c9-9ab9-8c35c69e67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa la tabla Q con valores arbitrarios\n",
    "Q = np.zeros((len(env.state_space), len(env.action_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2360ccc9-f8be-49c3-8321-d972a3f03e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los parámetros del algoritmo\n",
    "alpha = 0.1 # Tasa de aprendizaje\n",
    "gamma = 0.9 # Factor de descuento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8aca52-07e7-4718-be58-6fd084b1efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrena el agente utilizando Q-Learning\n",
    "for _ in range(1000):\n",
    "    state = np.random.choice(env.state_space)  # Estado inicial aleatorio\n",
    "    \n",
    "    while state != 3:  # Hasta llegar al estado objetivo\n",
    "        action = np.random.choice(env.action_space)  # Selecciona una acción aleatoria\n",
    "        next_state = state + action  # Determina el siguiente estado\n",
    "        reward = env.rewards[next_state]  # Obtiene la recompensa del siguiente estado\n",
    "        \n",
    "        # Actualiza la tabla Q\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "        state = next_state  # Actualiza el estado actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b834e95a-5041-423d-8edf-9645f09c4902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función Q-Valor aprendida :\n",
      "[[ 4.58  6.2 ]\n",
      " [ 6.2   8.  ]\n",
      " [ 8.   10.  ]\n",
      " [ 0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#Muestra la función Q-valor aprendida\n",
    "print(\"Función Q-Valor aprendida :\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9927d559-24e8-4002-8cd1-c841b6ba8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio 3: Sarsa\n",
    "#Reinicializa la tabla Q con valores arbitrarios\n",
    "Q = np.zeros((len(env.state_space), len(env.action_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44d85b3-e4f1-4b96-a4ad-a2272b4fbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena el agente utilizando Sarsa\n",
    "for _ in range(1000):\n",
    "    state = np.random.choice(env.state_space) # Estado inicial aleatorio\n",
    "    action = np.random.choice (env.action_space) # Selecciona una acción aleatoria\n",
    "    while state != 3: # Hasta llegar al estado objetivo\n",
    "        next_state = state + action\n",
    "        next_action = np.random.choice (env.action_space) # Selecciona una acción aleatorial\n",
    "        reward = env.rewards [next_state]\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "        state = next_state\n",
    "        action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f7f717-5ff1-45f0-9640-27088f5900b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función Q-Valor aprendida con Sarsa:\n",
      "[[ 0.72631831  3.47159331]\n",
      " [ 3.6083861   6.81067443]\n",
      " [ 6.16207759 10.        ]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Muestra la función Q-valor aprendida con Sarsa\n",
    "print(\"Función Q-Valor aprendida con Sarsa:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb26d2ea-4795-4615-97c2-574c313e03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 4: Política de Gradiente de Montecarlo\n",
    "# Inicializa la política con probabilidades uniformes\n",
    "policy= np.ones((len(env.state_space), len (env.action_space))) / len(env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d9aebe-98cf-4dd7-8d49-eacf1aae809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8a401ad-0109-48b6-8f5e-8558bfc8144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la función de recompensa promedio\n",
    "def average_reward(Q):\n",
    "    return np.mean([Q[state, np.argmax(policy [state])] for state in env.state_space])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6315fd20-d653-43a4-a808-789e6f22ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "# los valores de policy[state] deben sumar exactamente 1. Si no es así, hhay que normalizar las probabilidades:\n",
    "policy[state] = policy[state] / np.sum(policy[state])\n",
    "print(policy[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b2a69-ab1d-4696-8030-8f5ce19dccb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d00be1a-b0f3-47aa-b643-6b71d981c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena la política utilizando Gradiente de Montecarlo\n",
    "for _ in range(1000):\n",
    "    state = np.random.choice(env.state_space)  # Estado inicial aleatorio\n",
    "    while state != 3:  # Hasta llegar al estado objetivo\n",
    "        \n",
    "        # Evitar probabilidades negativas y normalizar la política antes de elegir acción\n",
    "        policy[state] = np.maximum(policy[state], 0)  # Evita probabilidades negativas\n",
    "        policy[state] = policy[state] / np.sum(policy[state])  # Asegura que sumen 1\n",
    "        \n",
    "        # Verificar que las probabilidades sean válidas\n",
    "        if np.any(policy[state] < 0):\n",
    "            raise ValueError(f\"Las probabilidades contienen valores negativos en el estado {state}: {policy[state]}\")\n",
    "        if not np.isclose(np.sum(policy[state]), 1):\n",
    "            raise ValueError(f\"Las probabilidades no suman 1 en el estado {state}: {policy[state]}\")\n",
    "\n",
    "        action = np.random.choice(env.action_space, p=policy[state])  # Selecciona una acción con la política actual\n",
    "        next_state = state + action\n",
    "        reward = env.rewards[next_state]\n",
    "        \n",
    "        gradient = np.zeros_like(policy[state])\n",
    "        gradient[action] = 1\n",
    "        alpha = 0.01  # Tasa de aprendizaje\n",
    "        \n",
    "        # Actualiza la política\n",
    "        policy[state] += alpha * gradient * (reward - average_reward(Q))\n",
    "        \n",
    "        # Evitar probabilidades negativas y normalizar después de la actualización\n",
    "        policy[state] = np.maximum(policy[state], 0)\n",
    "        policy[state] = policy[state] / np.sum(policy[state])  # Normaliza la política nuevamente\n",
    "\n",
    "        # Verificar nuevamente después de la actualización\n",
    "        if np.any(policy[state] < 0):\n",
    "            raise ValueError(f\"Las probabilidades contienen valores negativos en el estado {state}: {policy[state]}\")\n",
    "        if not np.isclose(np.sum(policy[state]), 1):\n",
    "            raise ValueError(f\"Las probabilidades no suman 1 en el estado {state}: {policy[state]}\")\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2abbadb1-dcd5-49cc-8c54-446d3f2e0302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica aprendida con Gradiente de Montecarlo:\n",
      "[[0.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [2.73068119e-17 1.00000000e+00]\n",
      " [5.00000000e-01 5.00000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Muestra la política aprendida\n",
    "print(\"Politica aprendida con Gradiente de Montecarlo:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6080717-7c9c-4c3b-99ea-0fecd9f26630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
